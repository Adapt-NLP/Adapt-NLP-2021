<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Adapt-NLP</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">Adapt</span><span class="title3">-</span><span class="title2">NLP</span> <span class="year">2021</span>
        </div>
        <div class="bottom-right">
            June 31, 2021 <br> Conference name, City Name
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Workshops Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a title="Program" href="program">Program</a> 
            </td>
            <td class="navigation">
                <a title="Call For Papers" href="call-for-papers">Call For<br>Papers</a> 
            </td>
            <td class="navigation">
                <a title="Program Commitee" href="pc">Program<br>Commitee</a>
            </td>
            <td class="navigation">
                <a title="Organizing Team" href="organizing-team">Organizing<br>Team</a>
            </td>
            <td class="navigation">
                <a title="Submitt a paper for the workshop" href="submissions">Submissions</a>
            </td>
        </tr>
    </table>

    <h2>Adapt-NLP: The Second Workshop on Domain Adaptation for NLP</h2>
    <p>
        Recent growth in computational abilities and the rise of Deep Neural Networks (DNNs) have pushed the frontier
        in Natural Language Processing (NLP) research. The ability to collect massive amounts of data with the capacity
        to train large models on powerful GPUs, which are now widely accessible, has facilitated many NLP applications. 
        However, many NLP algorithms rely on a fundamental assumption that the training and test sets follow the same underlying distribution. 
        When these distributions do not match, we are facing aphenomenon known as domain shift, 
        where such models are likely to encounter performance drops. 
        Despite the growing capabilities of data collection, domain shifts are very common, 
        since many NLP domains still lack a large quantity of labeled data to feed data-hungry neural models, 
        and in some, even unlabeled data is scarce. As a result, the concept of domain adaptation, 
        training an algorithmon annotated data from a source domain and applying it to other target domains, 
        is a long standing research in NLP which aims to narrow the domain shift.
    </p>
<!--     <p>
        You can use this template by cloning all the files from this
        <a target="_blank" href="https://github.com/mikepierce/conference-website-template">GitHub repo here</a>.
        This template was created from a website I made for 
        <a target="_blank" href="http://math.ucr.edu/~mathconn/">MathConnections 2019</a>,
        in case you want to see an example of this template being used in the wild.
    </p>
    <p>
        This paragraph is just some filler text to give the homepage some volume:
        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim et est et euismod. Fusce et metus tempus, pellentesque ex at, convallis nulla. Ut fringilla commodo tincidunt. Fusce sed est eu massa placerat iaculis eu at mauris. Nullam ut mollis nisi, quis malesuada risus. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam ipsum tortor, suscipit non tincidunt vel, bibendum in libero. Nulla facilisi. Pellentesque vitae neque metus. Cras quis est pharetra, vestibulum nisl et, viverra ipsum. Etiam porta dignissim purus, quis tempor metus volutpat eu. Praesent pulvinar libero eget purus tincidunt finibus.
    </p>
 -->
    <h2>Workshop Goals</h2>
    <p>
        This workshop aims to bring together researchers and ideas in the broad spectrum of NLP research in domain adaptation,
        zero-shot transfer learning and related setups where the data distribution differs between the train and test phases.
We believe there is much to gain by organizing this workshop, gathering a forum to share insights and discuss issues that arecommon across them. 
        We plan to focus on topics such as unsupervised and semi-supervised approaches to adaptation and transfer, 
        as well as the similarities and differences between domain adaptation and cross-lingual learning. 
        We would also like to explore the definitions of fundamental con-cepts such as the concepts of domain, 
        transfer aswell as zero-shot and few-shot learning.
    </p>
    
    <h2>Main Workshop Topics</h2>
    <ul>
        <li>Introducing algorithms for DA, focusing on unsupervised and semi-supervised approaches.</li> <br>
        <li>Similarities and differences between DA and cross-lingual learning.</li> <br>
        <li>Definitions of fundamental concepts such as the concepts of domain, transfer as well as zero-shot and few-shot learning.</li> <br>
        <li>Zero-shot and few-shot learning algorithms and their resemblance to unsupervised-DA.</li> <br>
        <li>Introducing new setup definitions for DA tasks. Aiming toward realistic and applicable scenarios, such as one-to-many DA, many-to-many DA and DA when the target domain is unknown.</li> <br>
        <li>New tasks and new domains that have not yet been explored in the proposed setups.</li> <br>
    </ul>
        
    
    <h2>Invited Speakers</h2>
    
    <table class="sponsors">
        <tr>
            <td class="sponsor">
                <figure>
                    <img src="assets/HS_scaled.png" alt="Hinrich Schutze, University of Munich (LMU)" style="width:300px;height:300px;">
                    <figcaption>Hinrich Schutze <br> University of Munich (LMU)</figcaption>
                </figure>
            </td>
            <td class="sponsor">
                <figure>
                    <img src="assets/JE_scaled.jpg" alt="Jacob Eisenstein, Google AI" style="width:300px;height:300px;">
                    <figcaption>Jacob Eisenstein <br> Google AI</figcaption>
                </figure>
            </td>
            <td class="sponsor">
                <figure>
                    <img src="assets/IA_scaled.jpeg" alt="Isabelle Augenstein, University of Copenhagen" style="width:300px;height:300px;">
                    <figcaption>Isabelle Augenstein <br> University of Copenhagen</figcaption>
                </figure>
            </td>
        </tr>
    </table>
    
    <h2>Panel Discussion</h2>
    
    <table class="sponsors">
        <tr>
            <td class="sponsor">
                <figure>
                    <img src="assets/RR_scaled.jpg" alt="Roi Reichart, Technion - Israel Institute of Technology." style="width:250px;height:250px;">
                    <figcaption>Roi Reichart <br> Technion - Israel Institute of Technology</figcaption>
                </figure>
            </td>
            <td class="sponsor">
                <figure>
                    <img src="assets/JE_scaled.jpg" alt="Jacob Eisenstein, Google AI" style="width:250px;height:250px;">
                    <figcaption>Jacob Eisenstein <br> Google AI</figcaption>
                </figure>
            </td>
            <td class="sponsor">
                <figure>
                    <img src="assets/AS_scaled.jpg" alt="Anders Søgaard, University of Copenhagen" style="width:250px;height:250px;">
                    <figcaption>Anders Søgaard <br> University of Copenhagen</figcaption>
                </figure>
            </td>
            <td class="sponsor">
                <figure>
                    <img src="assets/SR_scaled.png" alt="Sebastian Ruder, DeepMind" style="width:250px;height:250px;">
                    <figcaption>Sebastian Ruder <br> DeepMind</figcaption>
                </figure>
            </td>
        </tr>
    </table>


    <table class="footer">
        <tr>
            <td class="footer">Design by <br> <a target="_blank" href="https://eyalbd2.github.io/">Eyal Ben-David</a></td> 
            <td class="footer">&copy; Eyal Ben-David</td> 
        </tr>
    </table>

    </body>
</html>

