<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Accepted Papers | Adapt-NLP</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title2">Adapt-NLP</span> <span class="year">2021</span>
        </div>
        <div class="banner-center">
            April 20, 2021, EACL, Kyiv
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Workshops Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a title="Program" href="program">Program</a> 
            </td>
            <td class="navigation">
                <a title="Call For Papers" href="call-for-papers">Call For<br>Papers</a> 
            </td>
            <td class="navigation">
                <a class="current" title="Papers" href="papers">Papers</a> 
            </td>
            <td class="navigation">
                <a title="Program Commitee" href="pc">Program<br>Commitee</a>
            </td>
            <td class="navigation">
                <a title="Organizing Team" href="organizing-team">Organizing<br>Team</a>
            </td>
<!--             <td class="navigation">
                <a title="Submitt a paper for the workshop" href="submissions">Submissions</a>
            </td> -->
        </tr>
    </table>


    <h2>Accepted Papers</h2>
    
    <h4>Archival Track - Long Papers</h4>
    
    <table>
        <tr>
            <td>
                <b><big>Conditional Adversarial Networks for Multi-Domain Text Classification</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Yuan Wu, Diana Inkpen and Ahmed El-Roby </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Mikael Brunila, Rosie Zhao, Andrei Mircea Romascanu, Sam Lumley and Renee Sieber </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Genres, Parsers, and BERT: The Interaction Between Parsers and BERT Models in Cross-Genre Constituency Parsing in English and Swedish</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Daniel Dakota </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Cross-Lingual Transfer with MAML on Trees</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Jezabel Garcia, Federica Freddi, Jamie McGowan, Tim Nieradzik, Feng-Ting Liao, Ye Tian, Da-shan Shiu and Alberto Bernacchia </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Dario Stojanovski and Alexander Fraser </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>MultiReQA: A Cross-Domain Evaluation forRetrieval Question Answering Models</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Mandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen and Noah Constant </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>BERTologiCoMix: How does Code-Mixing interact with Multilingual BERT?</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Sebastin Santy, Anirudh Srinivasan and Monojit Choudhury </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Locality Preserving Loss: Neighbors that Live together, Align together</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Ashwinkumar Ganesan, Francis Ferraro and Tim Oates  </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Gordon Buck and Andreas Vlachos </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Dependency Parsing Evaluation for Low-resource Spontaneous Speech  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Zoey Liu and Emily Prud'hommeaux </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> User Factor Adaptation for User Embedding via Multitask Learning  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Xiaolei Huang, Michael J. Paul, Franck Dernoncourt, Robin Burke and Mark Dredze </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Abhinav Ramesh Kashyap, Laiba Mehnaz, Bhavitvya Malik, Abdul Waheed, Devamanyu Hazarika, Min-Yen Kan and Rajiv Ratn Shah </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Sanjeev Kumar Karn, Francine Chen, Yan-Ying Chen, Ulli Waltinger and Hinrich Schütze </i>
            </td>
        </tr>
    </table>
    
    
    
    
    
    <h4>Archival Track - Short Papers</h4>
    
    <table>
        <tr>
            <td>
                <b><big>Multidomain Pretrained Language Models for Green NLP</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Antonios Maronikolakis and Hinrich Schütze</i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Pseudo-Label Guided Unsupervised Domain Adaptation of Contextual Embeddings</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Tianyu Chen, Shaohan Huang, Furu Wei and Jianxin Li </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>The impact of domain-specific representations on BERT-based multi-domain spoken language understanding</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Judith Gaspers, Quynh Do, Tobias Röding and Melanie Bradford </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Anouck Braggaar and Rob van der Goot </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Domain adaptation in practice: Lessons from a real-world information extraction pipeline</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Timothy Miller, Egoitz Laparra and Steven Bethard </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> On the Hidden Negative Transfer in Sequential Transfer Learning for Domain Adaptation from News to Tweets  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Sara Meftah, Nasredine Semmar, Youssef Tamaazousti, Hassane Essafi and Fatiha Sadat </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>  An Empirical Study of Compound PCFGs </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Yanpeng Zhao and Ivan Titov </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Rob van der Goot, Ahmet Üstün and Barbara Plank </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Effective Distant Supervision for Temporal Relation Extraction  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Xinyu Zhao, Shih-Ting Lin and Greg Durrett </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Haoran Xu and Philipp Koehn </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Gradual Fine-Tuning for Low-Resource Domain Adaptation  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Haoran Xu, Seth Ebner, Mahsa Yarmohammadi, Aaron Steven White, Benjamin Van Durme and Kenton Murray </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>  Semantic Parsing of Brief and Multi-Intent Natural Language Utterances </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Logan Lebanoff, Charles Newton, Victor Hung, Beth Atkinson, John Killilea and Fei Liu </i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big> Domain Adaptation for NMT via Filtered Iterative Back-Translation  </big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i> Surabhi Kumari, Nikhil Jaiswal, Mayur Patidar, Manasi Patwardhan, Shirish Karande, Puneet Agarwal and Lovekesh Vig </i>
            </td>
        </tr>
    </table>
    
    
    <h4>Non-Archival Track</h4>
    
    <table>
        <tr>
            <td>
                <b><big>Training and Domain Adaptation for Supervised Text Segmentation</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Goran Glavaš, Ananya Ganesh and Swapna Somasundaran</i>
            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td>
                <b><big>Adapting to the Long Tail in Language Understanding</big></b>
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <i>Aakanksha Naik and Carolyn Rose</i>
            </td>
        </tr>
    </table>
    
    
    <h2>Oral Presentation</h2>
    
    <ul>
        <li> Conditional Adversarial Networks for Multi-Domain Text Classification </li> 
        <li> Dependency Parsing Evaluation for Low-resource Spontaneous Speech </li> 
        <li> Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data </li> 
        <li> Semantic Parsing of Brief and Multi-Intent Natural Language Utterances </li> 
        <li> Locality Preserving Loss: Neighbors that Live together, Align together </li> 
    </ul>

    
    
    
    <h2>Posters</h2>
    Since the workshop participants come from different time zones and because of the virtual nature of the conference, 
    the poster session occur twice and is accessible for any time zone. 
    
    <h4 id="poster_1">First Poster Session - 10:00am - 11:00am (CET) </h4>
    
    <ul>
        <li> Pseudo-Label Guided Unsupervised Domain Adaptation of Contextual Embeddings </li> 
        <li> The impact of domain-specific representations on BERT-based multi-domain spoken language understanding </li>
        <li> Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data </li>
        <li> Genres, Parsers, and BERT: The Interaction Between Parsers and BERT Models in Cross-Genre Constituency Parsing in English and Swedish </li>
        <li> Cross-Lingual Transfer with MAML on Trees </li>
        <li> Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation </li>
        <li> Domain adaptation in practice: Lessons from a real-world information extraction pipeline </li>
        <li> BERTologiCoMix: How does Code-Mixing interact with Multilingual BERT? </li>
        <li> Training and Domain Adaptation for Supervised Text Segmentation </li>
        <li> On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions </li>
        <li> Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation </li>
        <li> Gradual Fine-Tuning for Low-Resource Domain Adaptation </li>
        <li> Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer </li>
        <li> Domain Adaptation for NMT via Filtered Iterative Back-Translation </li>
    </ul>
    
    

    <h4 id="poster_2">Second Poster Session - 3:40pm - 4:40pm (CET) </h4>
    
    <ul>
        <li> Multidomain Pretrained Language Models for Green NLP </li>
        <li> The impact of domain-specific representations on BERT-based multi-domain spoken language understanding </li>
        <li> Bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management </li>
        <li> Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data </li>
        <li> Cross-Lingual Transfer with MAML on Trees </li>
        <li> Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation </li>
        <li> MultiReQA: A Cross-Domain Evaluation forRetrieval Question Answering Models </li>
        <li> Domain adaptation in practice: Lessons from a real-world information extraction pipeline </li>
        <li> On the Hidden Negative Transfer in Sequential Transfer Learning for Domain Adaptation from News to Tweets </li>
        <li> Training and Domain Adaptation for Supervised Text Segmentation </li>
        <li> Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning </li>
        <li> An Empirical Study of Compound PCFGs </li>
        <li> User Factor Adaptation for User Embedding via Multitask Learning </li>
        <li> On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions </li>
        <li> Effective Distant Supervision for Temporal Relation Extraction </li>
        <li> Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation </li>
        <li> Gradual Fine-Tuning for Low-Resource Domain Adaptation </li>
        <li> Adapting to the Long Tail in Language Understanding </li>
        <li> Domain Adaptation for NMT via Filtered Iterative Back-Translation </li>
    </ul>
    

    
    <table class="footer">
        <tr>
            <td class="footer">Maintined by<br> <a target="_blank" href="https://eyalbd2.github.io/">Eyal Ben-David</a></td> 
        </tr>
    </table>

    </body>
</html>
